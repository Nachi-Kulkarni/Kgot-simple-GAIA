#!/usr/bin/env python3
"""
Advanced RAG-MCP Search System - Task 29 Implementation

Implementation of enhanced RAG-MCP Section 3.2 semantic similarity search for MCP discovery
with context-aware recommendations, MCP composition suggestions, and cross-domain transfer capabilities.

This module extends the existing RAG-MCP framework with advanced capabilities:
1. Enhanced semantic similarity search with multi-hop reasoning for MCP discovery
2. Context-aware MCP recommendation system based on user patterns and task analysis
3. MCP composition suggestions for complex tasks with workflow optimization
4. Cross-domain MCP transfer capabilities for knowledge adaptation

Features:
- Advanced semantic search with graph-based reasoning
- LangChain-based intelligent agents for context analysis
- Real-time recommendation system with learning capabilities
- Complex task decomposition and MCP workflow orchestration
- Cross-domain knowledge transfer and adaptation
- OpenRouter integration for LLM-powered analysis
- Winston-compatible logging for comprehensive workflow tracking

@module AdvancedRAGMCPSearch
@author Enhanced Alita KGoT System
@version 1.0.0
@based_on RAG-MCP Section 3.2, Task 29 Requirements
"""

import os
import sys
import json
import logging
import asyncio
import aiohttp
import numpy as np
import pickle
import hashlib
import time
import networkx as nx
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Tuple, Callable, Set
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict, Counter
import math

# LangChain imports for agent orchestration (per user's hard rule)
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import BaseTool, Tool
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.schema import BaseMessage
from langchain_core.runnables import Runnable

# Scientific computing for advanced vector operations
try:
    import faiss
    FAISS_AVAILABLE = True
except ImportError:
    FAISS_AVAILABLE = False
    logging.warning("FAISS not available, using fallback similarity search")

try:
    import scipy.spatial.distance as distance
    from scipy.sparse import csr_matrix
    from scipy.sparse.csgraph import shortest_path
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    logging.warning("SciPy not available, using simplified algorithms")

# Pydantic for data validation
from pydantic import BaseModel, Field, validator

# Import existing RAG-MCP components for integration
sys.path.append(str(Path(__file__).parent.parent))
from alita_core.rag_mcp_engine import (
    RAGMCPEngine, MCPToolSpec, MCPCategory, MCPRetrievalResult, 
    ParetoMCPRegistry, RAGMCPValidator, VectorIndexManager
)
from alita_core.mcp_knowledge_base import (
    MCPKnowledgeBase, EnhancedMCPSpec, EnhancedVectorStore, 
    MCPPerformanceTracker, MCPQualityScore
)

# Setup Winston-compatible logging for Python with comprehensive workflow tracking
project_root = Path(__file__).parent.parent
log_dir = project_root / 'logs' / 'rag_enhancement'
log_dir.mkdir(parents=True, exist_ok=True)

# Configure logging with Winston-style formatting and multiple levels
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [%(operation)s] %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'advanced_rag_mcp_search.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('AdvancedRAGMCPSearch')

class SearchComplexity(Enum):
    """Complexity levels for search operations and MCP composition"""
    SIMPLE = "simple"          # Single MCP, straightforward task
    MODERATE = "moderate"      # 2-3 MCPs, some coordination needed
    COMPLEX = "complex"        # 4+ MCPs, complex workflow orchestration
    ENTERPRISE = "enterprise"  # Large-scale, multi-domain workflows

class RecommendationType(Enum):
    """Types of MCP recommendations generated by the system"""
    DIRECT = "direct"              # Direct task-relevant MCPs
    COMPLEMENTARY = "complementary" # MCPs that work well together
    ALTERNATIVE = "alternative"    # Alternative approaches to same task
    ENHANCEMENT = "enhancement"    # MCPs to enhance current workflow
    CROSS_DOMAIN = "cross_domain"  # MCPs from related domains

class CompositionStrategy(Enum):
    """Strategies for MCP composition and workflow orchestration"""
    SEQUENTIAL = "sequential"      # Step-by-step execution
    PARALLEL = "parallel"         # Simultaneous execution
    CONDITIONAL = "conditional"    # Decision-based branching
    HYBRID = "hybrid"             # Mixed approach optimization

@dataclass
class AdvancedSearchContext:
    """
    Enhanced search context for context-aware MCP discovery and recommendations
    Captures user intent, task complexity, and workflow requirements
    """
    # User and session context (required fields first)
    user_id: str
    session_id: str
    original_query: str
    task_domain: str
    
    # Optional fields with defaults
    timestamp: datetime = field(default_factory=datetime.now)
    complexity_level: SearchComplexity = SearchComplexity.SIMPLE
    estimated_duration: Optional[float] = None  # minutes
    
    # User preferences and history
    historical_preferences: Dict[str, Any] = field(default_factory=dict)
    current_workflow: List[str] = field(default_factory=list)
    preferred_categories: List[MCPCategory] = field(default_factory=list)
    
    # Context-specific requirements
    resource_constraints: Dict[str, Any] = field(default_factory=dict)
    quality_requirements: Dict[str, float] = field(default_factory=dict)
    performance_requirements: Dict[str, float] = field(default_factory=dict)
    
    # Multi-modal context information
    context_embeddings: Dict[str, np.ndarray] = field(default_factory=dict)
    semantic_tags: List[str] = field(default_factory=list)
    domain_concepts: List[str] = field(default_factory=list)

@dataclass
class MCPComposition:
    """
    MCP composition for complex task execution with workflow orchestration
    Represents a coordinated set of MCPs for achieving complex objectives
    """
    # Required fields first
    composition_id: str
    name: str
    description: str
    mcps: List[MCPToolSpec]
    execution_graph: Dict[str, List[str]]  # MCP dependency graph
    execution_strategy: CompositionStrategy
    estimated_completion_time: float  # minutes
    
    # Optional fields with defaults
    created_at: datetime = field(default_factory=datetime.now)
    
    # Resource and performance estimation
    resource_requirements: Dict[str, Any] = field(default_factory=dict)
    success_probability: float = 0.0
    cost_estimate: float = 0.0
    
    # Composition metadata
    complexity_score: float = 0.0
    optimization_score: float = 0.0
    reliability_score: float = 0.0
    
    # Workflow execution details
    critical_path: List[str] = field(default_factory=list)
    parallel_groups: List[List[str]] = field(default_factory=list)
    fallback_options: Dict[str, List[str]] = field(default_factory=dict)

@dataclass
class CrossDomainTransfer:
    """
    Cross-domain MCP transfer information for knowledge adaptation
    Enables MCPs to be adapted and transferred across different domains
    """
    # Required fields first
    transfer_id: str
    source_domain: str
    target_domain: str
    source_mcp: MCPToolSpec
    adapted_mcp: MCPToolSpec
    adaptation_confidence: float
    transfer_method: str
    
    # Optional fields with defaults
    concept_mappings: Dict[str, str] = field(default_factory=dict)
    adaptation_requirements: List[str] = field(default_factory=list)
    
    # Validation and effectiveness
    transfer_validation: Dict[str, Any] = field(default_factory=dict)
    effectiveness_score: float = 0.0
    
    # Usage and feedback
    usage_count: int = 0
    success_rate: float = 0.0
    user_feedback: List[Dict[str, Any]] = field(default_factory=list)

@dataclass
class AdvancedSearchResult:
    """
    Comprehensive search result with discovery, recommendations, and compositions
    Provides a complete analysis of MCP options for complex task requirements
    """
    # Required fields first
    search_id: str
    query: str
    search_context: AdvancedSearchContext
    primary_mcps: List[MCPRetrievalResult]
    recommended_mcps: List[MCPRetrievalResult]
    compositions: List[MCPComposition]
    cross_domain_suggestions: List[CrossDomainTransfer]
    
    # Optional fields with defaults
    timestamp: datetime = field(default_factory=datetime.now)
    confidence_scores: Dict[str, float] = field(default_factory=dict)
    recommendation_types: Dict[str, RecommendationType] = field(default_factory=dict)
    recommendation_explanations: Dict[str, str] = field(default_factory=dict)
    composition_rankings: Dict[str, float] = field(default_factory=dict)
    domain_similarity_scores: Dict[str, float] = field(default_factory=dict)
    
    # Search analytics and metadata
    context_metadata: Dict[str, Any] = field(default_factory=dict)
    search_performance: Dict[str, float] = field(default_factory=dict)
    total_processing_time: float = 0.0

class AdvancedMCPDiscoveryEngine:
    """
    Enhanced MCP discovery engine implementing advanced semantic search with multi-hop reasoning,
    graph-based MCP relationship modeling, and context-aware search optimization.
    
    This engine extends beyond basic semantic similarity to provide sophisticated MCP discovery
    capabilities including graph traversal, contextual embedding fusion, and dynamic search
    optimization based on user patterns and task requirements.
    """
    
    def __init__(self, 
                 knowledge_base: MCPKnowledgeBase,
                 openrouter_api_key: Optional[str] = None,
                 enable_graph_reasoning: bool = True,
                 max_reasoning_depth: int = 3,
                 similarity_threshold: float = 0.6):
        """
        Initialize Advanced MCP Discovery Engine with graph-based reasoning capabilities
        
        @param knowledge_base Existing MCP knowledge base for integration
        @param openrouter_api_key OpenRouter API key for LLM-powered analysis
        @param enable_graph_reasoning Enable graph-based multi-hop reasoning
        @param max_reasoning_depth Maximum depth for graph traversal reasoning
        @param similarity_threshold Minimum similarity for MCP discovery
        """
        self.knowledge_base = knowledge_base
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        self.enable_graph_reasoning = enable_graph_reasoning
        self.max_reasoning_depth = max_reasoning_depth
        self.similarity_threshold = similarity_threshold
        
        # Initialize MCP relationship graph for multi-hop reasoning
        self.mcp_graph = nx.Graph()
        self.capability_graph = nx.Graph()
        self.domain_graph = nx.Graph()
        
        # Initialize LLM client for context analysis (using OpenRouter per user memory)
        self.llm_client = self._initialize_llm_client()
        
        # Caching for performance optimization
        self.search_cache: Dict[str, Any] = {}
        self.relationship_cache: Dict[str, List[str]] = {}
        
        # Performance tracking
        self.discovery_metrics = {
            'total_searches': 0,
            'graph_reasoning_used': 0,
            'cache_hits': 0,
            'avg_search_time': 0.0
        }
        
        logger.info("Initialized Advanced MCP Discovery Engine", extra={
            'operation': 'DISCOVERY_ENGINE_INIT',
            'graph_reasoning_enabled': enable_graph_reasoning,
            'max_reasoning_depth': max_reasoning_depth,
            'similarity_threshold': similarity_threshold
        })
    
    def _initialize_llm_client(self) -> Optional[ChatOpenAI]:
        """
        Initialize OpenRouter LLM client for context analysis and reasoning
        
        @return Configured ChatOpenAI client or None if not available
        """
        try:
            if not self.openrouter_api_key:
                logger.warning("OpenRouter API key not provided", extra={
                    'operation': 'LLM_INIT_WARNING',
                    'component': 'AdvancedMCPDiscoveryEngine'
                })
                return None
            
            # Configure OpenRouter client for context analysis
            client = ChatOpenAI(
                openai_api_key=self.openrouter_api_key,
                openai_api_base="https://openrouter.ai/api/v1",
                model_name="anthropic/claude-sonnet-4",  # Use advanced model for reasoning
                temperature=0.3,  # Lower temperature for more focused analysis
                max_tokens=2048,
                timeout=30
            )
            
            logger.info("Initialized OpenRouter LLM client for discovery engine", extra={
                'operation': 'LLM_INIT_SUCCESS',
                'component': 'AdvancedMCPDiscoveryEngine',
                'model': "anthropic/claude-sonnet-4"
            })
            
            return client
            
        except Exception as e:
            logger.error(f"Failed to initialize LLM client: {str(e)}", extra={
                'operation': 'LLM_INIT_ERROR',
                'component': 'AdvancedMCPDiscoveryEngine',
                'error': str(e)
            })
            return None 

    async def initialize(self) -> bool:
        """
        Initialize the discovery engine with MCP relationship graphs and optimization
        
        @return Success status of initialization
        """
        try:
            logger.info("Initializing Advanced MCP Discovery Engine", extra={
                'operation': 'DISCOVERY_INIT_START',
                'component': 'AdvancedMCPDiscoveryEngine'
            })
            
            # Build MCP relationship graphs for multi-hop reasoning
            await self._build_mcp_relationship_graph()
            await self._build_capability_graph()
            await self._build_domain_graph()
            
            # Initialize search optimization
            await self._initialize_search_optimization()
            
            logger.info("Advanced MCP Discovery Engine initialized successfully", extra={
                'operation': 'DISCOVERY_INIT_SUCCESS',
                'component': 'AdvancedMCPDiscoveryEngine',
                'graph_nodes': self.mcp_graph.number_of_nodes(),
                'graph_edges': self.mcp_graph.number_of_edges()
            })
            
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize discovery engine: {str(e)}", extra={
                'operation': 'DISCOVERY_INIT_ERROR',
                'component': 'AdvancedMCPDiscoveryEngine',
                'error': str(e)
            })
            return False

    async def advanced_semantic_search(self, 
                                     query: str,
                                     search_context: AdvancedSearchContext,
                                     max_results: int = 10,
                                     enable_reasoning: bool = True) -> List[MCPRetrievalResult]:
        """
        Perform advanced semantic search with multi-hop reasoning and context awareness
        
        @param query Search query string
        @param search_context Enhanced search context with user preferences and requirements
        @param max_results Maximum number of results to return
        @param enable_reasoning Enable graph-based multi-hop reasoning
        @return List of enhanced MCP retrieval results with advanced scoring
        """
        search_start = time.time()
        search_id = str(uuid.uuid4())
        
        try:
            logger.info("Starting advanced semantic search", extra={
                'operation': 'ADVANCED_SEARCH_START',
                'search_id': search_id,
                'query': query[:100],
                'complexity': search_context.complexity_level.value,
                'reasoning_enabled': enable_reasoning
            })
            
            # Check cache for similar searches
            cache_key = self._generate_cache_key(query, search_context)
            if cache_key in self.search_cache:
                self.discovery_metrics['cache_hits'] += 1
                logger.debug("Cache hit for advanced search", extra={
                    'operation': 'SEARCH_CACHE_HIT',
                    'search_id': search_id,
                    'cache_key': cache_key[:20]
                })
                return self.search_cache[cache_key]
            
            # Step 1: Context-aware embedding generation
            context_embeddings = await self._generate_context_embeddings(query, search_context)
            
            # Step 2: Multi-modal semantic search
            base_results = await self._perform_multi_modal_search(
                query, context_embeddings, max_results * 2
            )
            
            # Step 3: Graph-based reasoning enhancement (if enabled)
            if enable_reasoning and self.enable_graph_reasoning:
                self.discovery_metrics['graph_reasoning_used'] += 1
                enhanced_results = await self._apply_graph_reasoning(
                    base_results, search_context, max_results
                )
            else:
                enhanced_results = base_results[:max_results]
            
            # Step 4: Context-aware scoring and ranking
            final_results = await self._apply_context_aware_scoring(
                enhanced_results, search_context
            )
            
            # Cache results for performance
            self.search_cache[cache_key] = final_results
            
            # Update performance metrics
            search_time = time.time() - search_start
            self._update_discovery_metrics(search_time)
            
            logger.info("Advanced semantic search completed", extra={
                'operation': 'ADVANCED_SEARCH_COMPLETE',
                'search_id': search_id,
                'results_count': len(final_results),
                'search_time_ms': search_time * 1000,
                'reasoning_used': enable_reasoning and self.enable_graph_reasoning
            })
            
            return final_results
            
        except Exception as e:
            logger.error(f"Advanced semantic search failed: {str(e)}", extra={
                'operation': 'ADVANCED_SEARCH_ERROR',
                'search_id': search_id,
                'error': str(e)
            })
            return []

    async def _build_mcp_relationship_graph(self):
        """
        Build MCP relationship graph based on capability overlap, usage patterns, and semantic similarity
        This graph enables multi-hop reasoning for discovering related MCPs
        """
        try:
            logger.info("Building MCP relationship graph", extra={
                'operation': 'GRAPH_BUILD_START',
                'component': 'AdvancedMCPDiscoveryEngine',
                'graph_type': 'mcp_relationships'
            })
            
            # Get all MCPs from knowledge base
            if hasattr(self.knowledge_base, 'mcp_registry'):
                mcps = list(self.knowledge_base.mcp_registry.values())
            else:
                logger.warning("No MCP registry found in knowledge base")
                return
            
            # Add MCPs as nodes
            for mcp in mcps:
                self.mcp_graph.add_node(mcp.name, mcp_spec=mcp)
            
            # Build edges based on relationships
            for i, mcp1 in enumerate(mcps):
                for j, mcp2 in enumerate(mcps[i+1:], i+1):
                    relationship_strength = await self._calculate_mcp_relationship(mcp1, mcp2)
                    if relationship_strength > 0.3:  # Threshold for meaningful relationships
                        self.mcp_graph.add_edge(
                            mcp1.name, mcp2.name, 
                            weight=relationship_strength,
                            relationship_type=self._determine_relationship_type(mcp1, mcp2)
                        )
            
            logger.info("MCP relationship graph built successfully", extra={
                'operation': 'GRAPH_BUILD_SUCCESS',
                'component': 'AdvancedMCPDiscoveryEngine',
                'nodes': self.mcp_graph.number_of_nodes(),
                'edges': self.mcp_graph.number_of_edges()
            })
            
        except Exception as e:
            logger.error(f"Failed to build MCP relationship graph: {str(e)}", extra={
                'operation': 'GRAPH_BUILD_ERROR',
                'component': 'AdvancedMCPDiscoveryEngine',
                'error': str(e)
            })

    async def _apply_graph_reasoning(self, 
                                   base_results: List[MCPRetrievalResult],
                                   search_context: AdvancedSearchContext,
                                   max_results: int) -> List[MCPRetrievalResult]:
        """
        Apply graph-based multi-hop reasoning to enhance search results
        
        @param base_results Initial search results from semantic search
        @param search_context Search context for reasoning guidance
        @param max_results Maximum number of results to return
        @return Enhanced results with graph-based reasoning
        """
        try:
            logger.debug("Applying graph-based reasoning", extra={
                'operation': 'GRAPH_REASONING_START',
                'base_results_count': len(base_results),
                'max_depth': self.max_reasoning_depth
            })
            
            enhanced_results = list(base_results)
            explored_mcps = {result.mcp_spec.name for result in base_results}
            
            # Multi-hop exploration from each base result
            for depth in range(1, self.max_reasoning_depth + 1):
                new_discoveries = []
                
                for result in base_results:
                    if result.mcp_spec.name in self.mcp_graph:
                        # Get neighboring MCPs at current depth
                        neighbors = self._get_graph_neighbors(
                            result.mcp_spec.name, depth, explored_mcps
                        )
                        
                        for neighbor_name, relationship_score in neighbors:
                            if neighbor_name not in explored_mcps:
                                neighbor_mcp = self.mcp_graph.nodes[neighbor_name]['mcp_spec']
                                
                                # Calculate reasoning-enhanced score
                                enhanced_score = self._calculate_reasoning_score(
                                    result, neighbor_mcp, relationship_score, depth, search_context
                                )
                                
                                if enhanced_score > self.similarity_threshold:
                                    reasoning_result = MCPRetrievalResult(
                                        mcp_spec=neighbor_mcp,
                                        similarity_score=enhanced_score,
                                        relevance_score=enhanced_score * 0.9,  # Slight discount for reasoning
                                        validation_score=result.validation_score * 0.95,
                                        selection_confidence=enhanced_score * 0.8,
                                        ranking_factors={
                                            'graph_reasoning': True,
                                            'reasoning_depth': depth,
                                            'relationship_score': relationship_score,
                                            'base_similarity': result.similarity_score
                                        }
                                    )
                                    
                                    new_discoveries.append(reasoning_result)
                                    explored_mcps.add(neighbor_name)
                
                enhanced_results.extend(new_discoveries)
            
            # Sort by enhanced scores and return top results
            enhanced_results.sort(key=lambda x: x.selection_confidence, reverse=True)
            
            logger.debug("Graph reasoning completed", extra={
                'operation': 'GRAPH_REASONING_COMPLETE',
                'total_results': len(enhanced_results),
                'new_discoveries': len(enhanced_results) - len(base_results)
            })
            
            return enhanced_results[:max_results]
            
        except Exception as e:
            logger.error(f"Graph reasoning failed: {str(e)}", extra={
                'operation': 'GRAPH_REASONING_ERROR',
                'error': str(e)
            })
            return base_results[:max_results]

class ContextAwareRecommendationSystem:
    """
    Context-aware MCP recommendation system implementing intelligent suggestions based on
    user patterns, task analysis, collaborative filtering, and real-time learning.
    
    This system provides personalized MCP recommendations that adapt to user preferences,
    analyze task requirements, and learn from usage patterns to improve recommendation quality.
    """
    
    def __init__(self, 
                 knowledge_base: MCPKnowledgeBase,
                 discovery_engine: AdvancedMCPDiscoveryEngine,
                 openrouter_api_key: Optional[str] = None,
                 learning_rate: float = 0.1,
                 recommendation_window_days: int = 30):
        """
        Initialize Context-Aware Recommendation System with learning capabilities
        
        @param knowledge_base MCP knowledge base for recommendation analysis
        @param discovery_engine Advanced discovery engine for enhanced search
        @param openrouter_api_key OpenRouter API key for LLM analysis
        @param learning_rate Learning rate for preference adaptation
        @param recommendation_window_days Time window for usage pattern analysis
        """
        self.knowledge_base = knowledge_base
        self.discovery_engine = discovery_engine
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        self.learning_rate = learning_rate
        self.recommendation_window_days = recommendation_window_days
        
        # User preference modeling
        self.user_preferences: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        self.usage_patterns: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
        self.collaborative_matrix: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        
        # Recommendation models
        self.category_preferences: Dict[str, Dict[MCPCategory, float]] = defaultdict(lambda: defaultdict(float))
        self.workflow_patterns: Dict[str, List[List[str]]] = defaultdict(list)
        self.success_patterns: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        
        # LangChain agent for intelligent analysis (per user's hard rule)
        self.recommendation_agent = self._initialize_recommendation_agent()
        
        # Performance tracking
        self.recommendation_metrics = {
            'total_recommendations': 0,
            'accepted_recommendations': 0,
            'user_satisfaction_score': 0.0,
            'learning_updates': 0
        }
        
        logger.info("Initialized Context-Aware Recommendation System", extra={
            'operation': 'RECOMMENDATION_INIT',
            'learning_rate': learning_rate,
            'window_days': recommendation_window_days
        })

    def _initialize_recommendation_agent(self) -> Optional[AgentExecutor]:
        """
        Initialize LangChain agent for intelligent recommendation analysis
        
        @return Configured AgentExecutor or None if not available
        """
        try:
            if not self.openrouter_api_key:
                logger.warning("OpenRouter API key not available for recommendation agent")
                return None
            
            # Initialize LLM client
            llm_client = ChatOpenAI(
                openai_api_key=self.openrouter_api_key,
                openai_api_base="https://openrouter.ai/api/v1",
                model_name="anthropic/claude-sonnet-4",
                temperature=0.4,
                max_tokens=1500
            )
            
            # Create recommendation analysis tools
            tools = [
                self._create_task_analysis_tool(),
                self._create_preference_analysis_tool(),
                self._create_workflow_optimization_tool()
            ]
            
            # Create agent prompt
            prompt = ChatPromptTemplate.from_messages([
                ("system", """You are an intelligent MCP recommendation agent. Your role is to analyze user tasks, 
                preferences, and usage patterns to provide personalized MCP recommendations that optimize workflow 
                efficiency and task completion success.
                
                Analyze user context, task requirements, and historical patterns to generate recommendations that are:
                1. Relevant to the current task and user goals
                2. Aligned with user preferences and past successful workflows
                3. Optimized for efficiency and resource utilization
                4. Complementary to create effective MCP compositions
                
                Use the available tools to perform deep analysis and provide actionable recommendations."""),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad")
            ])
            
            # Create agent
            agent = create_openai_functions_agent(llm_client, tools, prompt)
            agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)
            
            logger.info("Initialized recommendation LangChain agent", extra={
                'operation': 'RECOMMENDATION_AGENT_INIT',
                'tools_count': len(tools)
            })
            
            return agent_executor
            
        except Exception as e:
            logger.error(f"Failed to initialize recommendation agent: {str(e)}", extra={
                'operation': 'RECOMMENDATION_AGENT_ERROR',
                'error': str(e)
            })
            return None

    async def get_context_aware_recommendations(self,
                                              query: str,
                                              search_context: AdvancedSearchContext,
                                              primary_mcps: List[MCPRetrievalResult],
                                              max_recommendations: int = 5) -> Dict[RecommendationType, List[MCPRetrievalResult]]:
        """
        Generate context-aware MCP recommendations based on user patterns and task analysis
        
        @param query Original search query
        @param search_context Enhanced search context with user information
        @param primary_mcps Primary MCPs found through search
        @param max_recommendations Maximum recommendations per type
        @return Dictionary mapping recommendation types to MCP lists
        """
        try:
            logger.info("Generating context-aware recommendations", extra={
                'operation': 'RECOMMENDATION_START',
                'user_id': search_context.user_id,
                'query': query[:50],
                'primary_mcps_count': len(primary_mcps)
            })
            
            recommendations = {}
            
            # Generate different types of recommendations
            recommendations[RecommendationType.COMPLEMENTARY] = await self._get_complementary_recommendations(
                primary_mcps, search_context, max_recommendations
            )
            
            recommendations[RecommendationType.ALTERNATIVE] = await self._get_alternative_recommendations(
                query, search_context, primary_mcps, max_recommendations
            )
            
            recommendations[RecommendationType.ENHANCEMENT] = await self._get_enhancement_recommendations(
                primary_mcps, search_context, max_recommendations
            )
            
            recommendations[RecommendationType.CROSS_DOMAIN] = await self._get_cross_domain_recommendations(
                query, search_context, max_recommendations
            )
            
            # Update user preferences based on context
            await self._update_user_preferences(search_context, recommendations)
            
            # Track recommendation metrics
            self.recommendation_metrics['total_recommendations'] += sum(len(recs) for recs in recommendations.values())
            
            logger.info("Context-aware recommendations generated", extra={
                'operation': 'RECOMMENDATION_COMPLETE',
                'user_id': search_context.user_id,
                'recommendation_types': len(recommendations),
                'total_recommendations': sum(len(recs) for recs in recommendations.values())
            })
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Failed to generate recommendations: {str(e)}", extra={
                'operation': 'RECOMMENDATION_ERROR',
                'user_id': search_context.user_id,
                'error': str(e)
            })
            return {}

    async def _get_complementary_recommendations(self,
                                               primary_mcps: List[MCPRetrievalResult],
                                               search_context: AdvancedSearchContext,
                                               max_recommendations: int) -> List[MCPRetrievalResult]:
        """
        Get MCPs that work well together with the primary MCPs
        
        @param primary_mcps Primary MCPs for complementary analysis
        @param search_context Search context for user preferences
        @param max_recommendations Maximum number of recommendations
        @return List of complementary MCP recommendations
        """
        try:
            complementary_mcps = []
            
            # Analyze workflow patterns for complementary MCPs
            primary_names = [mcp.mcp_spec.name for mcp in primary_mcps]
            
            # Get frequently used together patterns
            for workflow in self.workflow_patterns.get(search_context.user_id, []):
                for primary_name in primary_names:
                    if primary_name in workflow:
                        # Find other MCPs in the same workflow
                        for other_mcp in workflow:
                            if other_mcp != primary_name:
                                # Calculate complementary score
                                comp_score = self._calculate_complementary_score(
                                    primary_name, other_mcp, search_context
                                )
                                
                                if comp_score > 0.6:
                                    # Get MCP spec and create result
                                    mcp_spec = await self._get_mcp_spec_by_name(other_mcp)
                                    if mcp_spec:
                                        comp_result = MCPRetrievalResult(
                                            mcp_spec=mcp_spec,
                                            similarity_score=comp_score,
                                            relevance_score=comp_score * 0.9,
                                            validation_score=0.8,
                                            selection_confidence=comp_score * 0.85,
                                            ranking_factors={
                                                'complementary_type': 'workflow_pattern',
                                                'co_occurrence_score': comp_score,
                                                'user_pattern_match': True
                                            }
                                        )
                                        complementary_mcps.append(comp_result)
            
            # Remove duplicates and sort by confidence
            seen_names = set()
            unique_complementary = []
            for mcp in complementary_mcps:
                if mcp.mcp_spec.name not in seen_names:
                    seen_names.add(mcp.mcp_spec.name)
                    unique_complementary.append(mcp)
            
            unique_complementary.sort(key=lambda x: x.selection_confidence, reverse=True)
            
            logger.debug("Generated complementary recommendations", extra={
                'operation': 'COMPLEMENTARY_RECOMMENDATIONS',
                'count': len(unique_complementary[:max_recommendations])
            })
            
            return unique_complementary[:max_recommendations]
            
        except Exception as e:
            logger.error(f"Failed to get complementary recommendations: {str(e)}", extra={
                'operation': 'COMPLEMENTARY_ERROR',
                'error': str(e)
            })
            return [] 

class MCPCompositionEngine:
    """
    MCP Composition Engine for complex task decomposition and workflow orchestration.
    
    This engine analyzes complex tasks, decomposes them into subtasks, and creates
    optimized MCP workflows with dependency resolution, parallel execution planning,
    and failure recovery strategies.
    """
    
    def __init__(self, 
                 knowledge_base: MCPKnowledgeBase,
                 discovery_engine: AdvancedMCPDiscoveryEngine,
                 openrouter_api_key: Optional[str] = None,
                 max_composition_size: int = 10,
                 optimization_iterations: int = 3):
        """
        Initialize MCP Composition Engine with workflow optimization capabilities
        
        @param knowledge_base MCP knowledge base for composition analysis
        @param discovery_engine Advanced discovery engine for MCP selection
        @param openrouter_api_key OpenRouter API key for LLM-powered task analysis
        @param max_composition_size Maximum number of MCPs in a single composition
        @param optimization_iterations Number of optimization iterations for workflows
        """
        self.knowledge_base = knowledge_base
        self.discovery_engine = discovery_engine
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        self.max_composition_size = max_composition_size
        self.optimization_iterations = optimization_iterations
        
        # Task decomposition and workflow analysis
        self.task_patterns: Dict[str, List[str]] = {}
        self.workflow_templates: Dict[str, Dict[str, Any]] = {}
        self.composition_cache: Dict[str, MCPComposition] = {}
        
        # LangChain agent for task decomposition (per user's hard rule)
        self.composition_agent = self._initialize_composition_agent()
        
        # Performance tracking
        self.composition_metrics = {
            'total_compositions': 0,
            'successful_compositions': 0,
            'avg_composition_time': 0.0,
            'optimization_improvements': 0
        }
        
        logger.info("Initialized MCP Composition Engine", extra={
            'operation': 'COMPOSITION_INIT',
            'max_size': max_composition_size,
            'optimization_iterations': optimization_iterations
        })

    def _initialize_composition_agent(self) -> Optional[AgentExecutor]:
        """
        Initialize LangChain agent for intelligent task decomposition and workflow planning
        
        @return Configured AgentExecutor or None if not available
        """
        try:
            if not self.openrouter_api_key:
                logger.warning("OpenRouter API key not available for composition agent")
                return None
            
            # Initialize LLM client
            llm_client = ChatOpenAI(
                openai_api_key=self.openrouter_api_key,
                openai_api_base="https://openrouter.ai/api/v1",
                model_name="anthropic/claude-sonnet-4",
                temperature=0.2,  # Lower temperature for more structured decomposition
                max_tokens=2048
            )
            
            # Create composition analysis tools
            tools = [
                self._create_task_decomposition_tool(),
                self._create_dependency_analysis_tool(),
                self._create_workflow_optimization_tool()
            ]
            
            # Create agent prompt
            prompt = ChatPromptTemplate.from_messages([
                ("system", """You are an expert MCP workflow composition agent. Your role is to analyze complex tasks, 
                decompose them into manageable subtasks, and create optimized MCP workflows that efficiently accomplish 
                the user's objectives.
                
                Your responsibilities include:
                1. Breaking down complex tasks into logical subtasks
                2. Identifying MCP requirements for each subtask
                3. Analyzing dependencies between subtasks
                4. Optimizing workflow execution strategies (sequential, parallel, conditional)
                5. Planning resource utilization and error recovery
                
                Focus on creating practical, efficient workflows that minimize execution time while maximizing success probability."""),
                MessagesPlaceholder(variable_name="chat_history"),
                ("human", "{input}"),
                MessagesPlaceholder(variable_name="agent_scratchpad")
            ])
            
            # Create agent
            agent = create_openai_functions_agent(llm_client, tools, prompt)
            agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)
            
            logger.info("Initialized composition LangChain agent", extra={
                'operation': 'COMPOSITION_AGENT_INIT',
                'tools_count': len(tools)
            })
            
            return agent_executor
            
        except Exception as e:
            logger.error(f"Failed to initialize composition agent: {str(e)}", extra={
                'operation': 'COMPOSITION_AGENT_ERROR',
                'error': str(e)
            })
            return None

    async def generate_mcp_compositions(self,
                                      query: str,
                                      search_context: AdvancedSearchContext,
                                      available_mcps: List[MCPRetrievalResult],
                                      max_compositions: int = 3) -> List[MCPComposition]:
        """
        Generate optimized MCP compositions for complex task execution
        
        @param query Original task query
        @param search_context Enhanced search context with requirements
        @param available_mcps Available MCPs for composition
        @param max_compositions Maximum number of compositions to generate
        @return List of optimized MCP compositions
        """
        try:
            logger.info("Generating MCP compositions", extra={
                'operation': 'COMPOSITION_START',
                'query': query[:50],
                'available_mcps': len(available_mcps),
                'complexity': search_context.complexity_level.value
            })
            
            compositions = []
            
            # Step 1: Task decomposition and analysis
            task_analysis = await self._analyze_task_complexity(query, search_context)
            subtasks = await self._decompose_task(query, task_analysis, search_context)
            
            # Step 2: MCP assignment to subtasks
            mcp_assignments = await self._assign_mcps_to_subtasks(subtasks, available_mcps)
            
            # Step 3: Generate composition strategies
            for strategy in [CompositionStrategy.SEQUENTIAL, CompositionStrategy.PARALLEL, CompositionStrategy.HYBRID]:
                if len(compositions) >= max_compositions:
                    break
                
                composition = await self._create_composition(
                    query, subtasks, mcp_assignments, strategy, search_context
                )
                
                if composition:
                    # Step 4: Optimize composition
                    optimized_composition = await self._optimize_composition(composition)
                    compositions.append(optimized_composition)
            
            # Step 5: Rank compositions by effectiveness
            ranked_compositions = await self._rank_compositions(compositions, search_context)
            
            # Update performance metrics
            self.composition_metrics['total_compositions'] += len(ranked_compositions)
            
            logger.info("MCP compositions generated", extra={
                'operation': 'COMPOSITION_COMPLETE',
                'compositions_count': len(ranked_compositions),
                'subtasks_identified': len(subtasks)
            })
            
            return ranked_compositions[:max_compositions]
            
        except Exception as e:
            logger.error(f"Failed to generate MCP compositions: {str(e)}", extra={
                'operation': 'COMPOSITION_ERROR',
                'error': str(e)
            })
            return []

    async def _decompose_task(self, 
                            query: str, 
                            task_analysis: Dict[str, Any],
                            search_context: AdvancedSearchContext) -> List[Dict[str, Any]]:
        """
        Decompose complex task into manageable subtasks with requirements analysis
        
        @param query Original task query
        @param task_analysis Task complexity analysis results
        @param search_context Search context for decomposition guidance
        @return List of subtask specifications
        """
        try:
            # Use LangChain agent for intelligent decomposition if available
            if self.composition_agent:
                agent_input = {
                    "input": f"""
                    Analyze and decompose this complex task into logical subtasks:
                    
                    Task: {query}
                    Complexity Level: {search_context.complexity_level.value}
                    Domain: {search_context.task_domain}
                    Estimated Duration: {search_context.estimated_duration or 'Unknown'}
                    
                    Task Analysis: {json.dumps(task_analysis, indent=2)}
                    
                    Provide a structured decomposition with:
                    1. List of subtasks with clear descriptions
                    2. Dependencies between subtasks
                    3. Estimated effort and priority for each subtask
                    4. Required capabilities for each subtask
                    """
                }
                
                try:
                    result = await self.composition_agent.ainvoke(agent_input)
                    agent_output = result.get('output', '')
                    
                    # Parse agent output into structured subtasks
                    subtasks = self._parse_agent_decomposition(agent_output)
                    
                    if subtasks:
                        logger.debug("Task decomposition completed using LangChain agent", extra={
                            'operation': 'TASK_DECOMPOSITION_AGENT',
                            'subtasks_count': len(subtasks)
                        })
                        return subtasks
                        
                except Exception as e:
                    logger.warning(f"Agent decomposition failed, using fallback: {str(e)}")
            
            # Fallback: Rule-based decomposition
            subtasks = self._rule_based_decomposition(query, task_analysis, search_context)
            
            logger.debug("Task decomposition completed using rule-based approach", extra={
                'operation': 'TASK_DECOMPOSITION_FALLBACK',
                'subtasks_count': len(subtasks)
            })
            
            return subtasks
            
        except Exception as e:
            logger.error(f"Task decomposition failed: {str(e)}", extra={
                'operation': 'TASK_DECOMPOSITION_ERROR',
                'error': str(e)
            })
            return []

class CrossDomainTransferSystem:
    """
    Cross-Domain MCP Transfer System for knowledge adaptation and domain bridging.
    
    This system enables MCPs to be adapted and transferred across different domains
    through concept mapping, transfer learning, and domain-specific optimization.
    """
    
    def __init__(self, 
                 knowledge_base: MCPKnowledgeBase,
                 discovery_engine: AdvancedMCPDiscoveryEngine,
                 openrouter_api_key: Optional[str] = None,
                 domain_similarity_threshold: float = 0.4,
                 transfer_confidence_threshold: float = 0.6):
        """
        Initialize Cross-Domain Transfer System with adaptation capabilities
        
        @param knowledge_base MCP knowledge base for domain analysis
        @param discovery_engine Advanced discovery engine for cross-domain search
        @param openrouter_api_key OpenRouter API key for domain analysis
        @param domain_similarity_threshold Minimum similarity for domain transfer
        @param transfer_confidence_threshold Minimum confidence for transfer suggestions
        """
        self.knowledge_base = knowledge_base
        self.discovery_engine = discovery_engine
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        self.domain_similarity_threshold = domain_similarity_threshold
        self.transfer_confidence_threshold = transfer_confidence_threshold
        
        # Domain knowledge and mapping
        self.domain_ontology: Dict[str, Set[str]] = defaultdict(set)
        self.concept_mappings: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
        self.transfer_history: List[CrossDomainTransfer] = []
        
        # Domain embedding spaces
        self.domain_embeddings: Dict[str, np.ndarray] = {}
        self.concept_embeddings: Dict[str, np.ndarray] = {}
        
        # LangChain agent for domain analysis (per user's hard rule)
        self.transfer_agent = self._initialize_transfer_agent()
        
        # Performance tracking
        self.transfer_metrics = {
            'total_transfers': 0,
            'successful_transfers': 0,
            'avg_transfer_confidence': 0.0,
            'domain_coverage': 0.0
        }
        
        logger.info("Initialized Cross-Domain Transfer System", extra={
            'operation': 'TRANSFER_INIT',
            'domain_threshold': domain_similarity_threshold,
            'confidence_threshold': transfer_confidence_threshold
        })

    async def find_cross_domain_mcps(self,
                                   query: str,
                                   search_context: AdvancedSearchContext,
                                   target_domain: str,
                                   max_suggestions: int = 5) -> List[CrossDomainTransfer]:
        """
        Find and adapt MCPs from related domains for cross-domain transfer
        
        @param query Original search query
        @param search_context Enhanced search context
        @param target_domain Target domain for MCP adaptation
        @param max_suggestions Maximum number of transfer suggestions
        @return List of cross-domain transfer suggestions
        """
        try:
            logger.info("Finding cross-domain MCP transfers", extra={
                'operation': 'CROSS_DOMAIN_START',
                'query': query[:50],
                'target_domain': target_domain,
                'source_domain': search_context.task_domain
            })
            
            transfers = []
            
            # Step 1: Identify related domains
            related_domains = await self._find_related_domains(
                search_context.task_domain, target_domain
            )
            
            # Step 2: Find MCPs in related domains
            for domain in related_domains:
                domain_mcps = await self._find_mcps_in_domain(domain, query)
                
                # Step 3: Evaluate transfer potential
                for mcp in domain_mcps:
                    transfer = await self._evaluate_transfer_potential(
                        mcp, search_context.task_domain, target_domain, query
                    )
                    
                    if transfer and transfer.effectiveness_score >= self.transfer_confidence_threshold:
                        transfers.append(transfer)
            
            # Step 4: Rank transfers by effectiveness
            transfers.sort(key=lambda x: x.effectiveness_score, reverse=True)
            
            # Update performance metrics
            self.transfer_metrics['total_transfers'] += len(transfers)
            
            logger.info("Cross-domain transfers found", extra={
                'operation': 'CROSS_DOMAIN_COMPLETE',
                'transfers_count': len(transfers[:max_suggestions]),
                'related_domains': len(related_domains)
            })
            
            return transfers[:max_suggestions]
            
        except Exception as e:
            logger.error(f"Cross-domain transfer search failed: {str(e)}", extra={
                'operation': 'CROSS_DOMAIN_ERROR',
                'error': str(e)
            })
            return []

class AdvancedRAGMCPSearchSystem:
    """
    Main coordinator class for Advanced RAG-MCP Search System implementing Task 29 requirements.
    
    This system integrates all advanced capabilities:
    1. Enhanced semantic similarity search for MCP discovery
    2. Context-aware MCP recommendation system
    3. MCP composition suggestions for complex tasks
    4. Cross-domain MCP transfer capabilities
    
    Provides a unified interface for sophisticated MCP search, recommendation, and composition.
    """
    
    def __init__(self, 
                 knowledge_base: MCPKnowledgeBase,
                 openrouter_api_key: Optional[str] = None,
                 enable_all_features: bool = True,
                 cache_size: int = 1000):
        """
        Initialize Advanced RAG-MCP Search System with all enhanced capabilities
        
        @param knowledge_base Existing MCP knowledge base for integration
        @param openrouter_api_key OpenRouter API key for LLM-powered analysis
        @param enable_all_features Enable all advanced features (discovery, recommendations, composition, transfer)
        @param cache_size Maximum number of cached search results
        """
        self.knowledge_base = knowledge_base
        self.openrouter_api_key = openrouter_api_key or os.getenv('OPENROUTER_API_KEY')
        self.enable_all_features = enable_all_features
        self.cache_size = cache_size
        
        # Initialize core components
        self.discovery_engine = AdvancedMCPDiscoveryEngine(
            knowledge_base=knowledge_base,
            openrouter_api_key=openrouter_api_key
        )
        
        self.recommendation_system = ContextAwareRecommendationSystem(
            knowledge_base=knowledge_base,
            discovery_engine=self.discovery_engine,
            openrouter_api_key=openrouter_api_key
        )
        
        self.composition_engine = MCPCompositionEngine(
            knowledge_base=knowledge_base,
            discovery_engine=self.discovery_engine,
            openrouter_api_key=openrouter_api_key
        )
        
        self.transfer_system = CrossDomainTransferSystem(
            knowledge_base=knowledge_base,
            discovery_engine=self.discovery_engine,
            openrouter_api_key=openrouter_api_key
        )
        
        # System state and caching
        self.search_cache: Dict[str, AdvancedSearchResult] = {}
        self.system_metrics = {
            'total_searches': 0,
            'successful_searches': 0,
            'avg_response_time': 0.0,
            'feature_usage': defaultdict(int)
        }
        
        logger.info("Initialized Advanced RAG-MCP Search System", extra={
            'operation': 'SYSTEM_INIT',
            'all_features_enabled': enable_all_features,
            'cache_size': cache_size
        })

    async def initialize(self) -> bool:
        """
        Initialize the complete Advanced RAG-MCP Search System
        
        @return Success status of initialization
        """
        try:
            logger.info("Initializing Advanced RAG-MCP Search System", extra={
                'operation': 'SYSTEM_INIT_START'
            })
            
            # Initialize all components
            discovery_success = await self.discovery_engine.initialize()
            
            if not discovery_success:
                logger.error("Failed to initialize discovery engine")
                return False
            
            logger.info("Advanced RAG-MCP Search System initialized successfully", extra={
                'operation': 'SYSTEM_INIT_SUCCESS',
                'components_initialized': 4
            })
            
            return True
            
        except Exception as e:
            logger.error(f"System initialization failed: {str(e)}", extra={
                'operation': 'SYSTEM_INIT_ERROR',
                'error': str(e)
            })
            return False

    async def execute_advanced_search(self,
                                    query: str,
                                    user_id: str = "default",
                                    session_id: Optional[str] = None,
                                    task_domain: str = "general",
                                    complexity_level: SearchComplexity = SearchComplexity.MODERATE,
                                    enable_recommendations: bool = True,
                                    enable_compositions: bool = True,
                                    enable_cross_domain: bool = True) -> AdvancedSearchResult:
        """
        Execute comprehensive advanced search with all RAG-MCP capabilities
        
        @param query Search query string
        @param user_id User identifier for personalization
        @param session_id Session identifier for context tracking
        @param task_domain Task domain for domain-specific optimization
        @param complexity_level Estimated task complexity level
        @param enable_recommendations Enable context-aware recommendations
        @param enable_compositions Enable MCP composition generation
        @param enable_cross_domain Enable cross-domain transfer suggestions
        @return Comprehensive advanced search results
        """
        search_start = time.time()
        search_id = str(uuid.uuid4())
        
        try:
            logger.info("Executing advanced RAG-MCP search", extra={
                'operation': 'ADVANCED_SEARCH_EXECUTE',
                'search_id': search_id,
                'query': query[:100],
                'user_id': user_id,
                'complexity': complexity_level.value,
                'features_enabled': {
                    'recommendations': enable_recommendations,
                    'compositions': enable_compositions,
                    'cross_domain': enable_cross_domain
                }
            })
            
            # Create enhanced search context
            search_context = AdvancedSearchContext(
                user_id=user_id,
                session_id=session_id or str(uuid.uuid4()),
                original_query=query,
                task_domain=task_domain,
                complexity_level=complexity_level
            )
            
            # Step 1: Advanced semantic discovery
            primary_mcps = await self.discovery_engine.advanced_semantic_search(
                query=query,
                search_context=search_context,
                max_results=10,
                enable_reasoning=True
            )
            
            self.system_metrics['feature_usage']['discovery'] += 1
            
            # Step 2: Context-aware recommendations (if enabled)
            recommendations = {}
            if enable_recommendations and self.enable_all_features:
                recommendations = await self.recommendation_system.get_context_aware_recommendations(
                    query=query,
                    search_context=search_context,
                    primary_mcps=primary_mcps,
                    max_recommendations=5
                )
                self.system_metrics['feature_usage']['recommendations'] += 1
            
            # Step 3: MCP compositions (if enabled)
            compositions = []
            if enable_compositions and self.enable_all_features and complexity_level != SearchComplexity.SIMPLE:
                compositions = await self.composition_engine.generate_mcp_compositions(
                    query=query,
                    search_context=search_context,
                    available_mcps=primary_mcps,
                    max_compositions=3
                )
                self.system_metrics['feature_usage']['compositions'] += 1
            
            # Step 4: Cross-domain transfers (if enabled)
            cross_domain_suggestions = []
            if enable_cross_domain and self.enable_all_features:
                cross_domain_suggestions = await self.transfer_system.find_cross_domain_mcps(
                    query=query,
                    search_context=search_context,
                    target_domain=task_domain,
                    max_suggestions=3
                )
                self.system_metrics['feature_usage']['cross_domain'] += 1
            
            # Step 5: Create comprehensive result
            total_time = time.time() - search_start
            
            result = AdvancedSearchResult(
                search_id=search_id,
                query=query,
                search_context=search_context,
                primary_mcps=primary_mcps,
                recommended_mcps=list(recommendations.values()) if recommendations else [],
                compositions=compositions,
                cross_domain_suggestions=cross_domain_suggestions,
                total_processing_time=total_time
            )
            
            # Cache result for performance
            if len(self.search_cache) < self.cache_size:
                cache_key = self._generate_result_cache_key(query, search_context)
                self.search_cache[cache_key] = result
            
            # Update system metrics
            self._update_system_metrics(total_time, True)
            
            logger.info("Advanced RAG-MCP search completed", extra={
                'operation': 'ADVANCED_SEARCH_COMPLETE',
                'search_id': search_id,
                'total_time_ms': total_time * 1000,
                'primary_mcps': len(primary_mcps),
                'recommendations': sum(len(recs) for recs in recommendations.values()) if recommendations else 0,
                'compositions': len(compositions),
                'cross_domain': len(cross_domain_suggestions)
            })
            
            return result
            
        except Exception as e:
            total_time = time.time() - search_start
            self._update_system_metrics(total_time, False)
            
            logger.error(f"Advanced search execution failed: {str(e)}", extra={
                'operation': 'ADVANCED_SEARCH_ERROR',
                'search_id': search_id,
                'error': str(e)
            })
            
            # Return minimal result on error
            return AdvancedSearchResult(
                search_id=search_id,
                query=query,
                search_context=search_context,
                primary_mcps=[],
                recommended_mcps=[],
                compositions=[],
                cross_domain_suggestions=[],
                total_processing_time=total_time
            ) 